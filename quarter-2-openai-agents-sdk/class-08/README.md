# ğŸ¤– OpenAI Agents SDK â€“ Class 8 (11 October 2025)

---

## ğŸ¤ **Handoffs** â€” Basic

**ğŸ“˜ Definition:**
A *handoff* is when your current agent **transfers control** to another, more specialized agent to finish the task.
In the SDK, a handoff is treated as a **tool** (e.g. `transfer_to_refund_agent`).

**ğŸ’¡ Why we need it:**
Used when different specialists handle different parts of a workflow â€” like **triage âœ billing âœ refund**.
Think customer support routing!

**ğŸ§  Analogy:**

* *Agents-as-tools* â†’ ask a colleague a quick question.
* *Handoff* â†’ transfer the whole call to that colleague.

**ğŸ§© Core SDK Concepts:**

* `Agent.handoffs`: list of agents or `handoff(...)` objects it can transfer to.
* `handoff(...)`: customize name, description, `on_handoff`, and filters.
* Handoff appears to the LLM as a **tool** (e.g. `transfer_to_agentname`).

**ğŸ“œ Example:**
A â€œTriage Agentâ€ routes user queries to **Billing** or **Refund** specialists.

```python
triage_agent = Agent(
    name="Triage agent",
    instructions="If billing âœ Billing agent. If refund âœ Refund agent.",
    handoffs=[billing_agent, handoff(refund_agent)],
)
```

**ğŸ§ª Lab:**
Change user input and inspect `result.new_items` to spot
`HandoffCallItem` and `HandoffOutputItem` â€” proof that handoff occurred.

---

## ğŸ§  **Advanced Handoffs**

**ğŸ’¬ Big Idea:**
Advanced handoffs = **VIP transfers** â€” next agent gets a *briefing*, clean history, and structured data.
Itâ€™s not just routing; itâ€™s orchestration.

**ğŸ§° What you can control:**

1. ğŸ· **Customize** tool name, description, or add callbacks (`on_handoff`).
2. ğŸ“¦ **Pass structured data** (Pydantic models like `EscalationData`).
3. ğŸ§¹ **Filter history** so the new agent only sees relevant context.
4. ğŸ” **Continue conversation** seamlessly with the right specialist.

**ğŸ§© Example:**

```python
custom_handoff = handoff(
    agent=specialist,
    tool_name_override="escalate_to_specialist",
    on_handoff=log_handoff_event,
)
```

**ğŸ§  Structured Handoff Example:**

```python
class EscalationData(BaseModel):
    reason: str
    order_id: str
```

This forces the LLM to pass `reason` and `order_id` during the transfer.

**ğŸ§¹ Clean History:**
Use `handoff_filters.remove_all_tools` to give the next agent a clean slate.

**ğŸ†š Handoffs vs Agents-as-tools:**

* ğŸ¤ Handoff â†’ long, multi-turn dialogs; next agent *owns* conversation.
* ğŸ›  Agent-as-tool â†’ quick subtask; main agent *keeps* control.

**âš  Tips:**

* Make handoff prompts explicit.
* Sanitize history.
* Use `result.last_agent` to continue conversation with same specialist.

---

## ğŸ§° **Agents as a Tool**

**ğŸ“˜ Meaning:**
Let one agent **call another like a function** â€” without losing control.
The *main agent* stays in charge while *specialist agents* do small jobs.

**ğŸ’¡ Why we need it:**

* ğŸ•¹ Main agent keeps control.
* ğŸ§© Modular, reusable sub-agents.
* ğŸ¯ Deterministic orchestration.
* âœ Clean, focused prompts for each specialist.

**ğŸ§  Analogy:**

* *Handoff* â†’ transfer the call.
* *Agent as a tool* â†’ put call on hold, ask a colleague, then you answer.

**âš™ SDK Support:**
Use `agent.as_tool()` to wrap one agent as a callable tool.
Or use a `@function_tool` that runs another agent via `Runner.run()` for full control.

**ğŸ“œ Example:**

```python
orchestrator = Agent(
    name="Translator Orchestrator",
    tools=[
        spanish_agent.as_tool("translate_to_spanish", "Translate to Spanish"),
        french_agent.as_tool("translate_to_french", "Translate to French"),
    ],
)
```

**ğŸ§  Advanced Pattern:**
Run a sub-agent *inside* a tool:

```python
@function_tool
async def proofread_text(text: str) -> str:
    result = await Runner.run(proofreader, text, max_turns=3)
    return result.final_output
```

**ğŸ§­ Choosing Between:**

| Scenario               | Use              |
| ---------------------- | ---------------- |
| Short, scoped subtasks | ğŸ§° Agent as Tool |
| Long, focused sessions | ğŸ¤ Handoff       |

**ğŸ•µ Gotchas:**

* `model_settings.tool_choice`: `"auto"`, `"required"`, `"none"`, or a specific tool.
* `tool_use_behavior="stop_on_first_tool"` to control loops.
* Use traces to debug tool calls.

---

## ğŸ› **Model Settings**

**ğŸ¯ What:**
Model settings are like **knobs and dials** controlling your agentâ€™s brain.
Tune creativity, response length, and tool usage.

**ğŸ§‘â€ğŸ³ Analogy:**

* *Temperature* â†’ creativity
* *Tool Choice* â†’ allow / disallow tools
* *Max Tokens* â†’ response length
* *Parallel Tools* â†’ use multiple tools at once

**âš™ Key Controls:**

1. **Temperature**

   * Low (0.1) = Focused
   * High (0.9) = Creative

   ```python
   ModelSettings(temperature=0.3)
   ```

2. **Tool Choice**

   * `"auto"` â€“ decide automatically
   * `"required"` â€“ must use a tool
   * `"none"` â€“ no tools

3. **Max Tokens**
   Limit response length.

4. **Parallel Tool Calls**
   Run multiple tools together or one-by-one.

**ğŸ§ª Examples:**

* ğŸ§® Math Tutor â†’ `temperature=0.1`
* âœ Story Writer â†’ `temperature=0.8`
* ğŸ§° Tool User â†’ `tool_choice="required"`

**ğŸ§  Advanced:**
Adjust `top_p`, `frequency_penalty`, `presence_penalty` for word variety.

**ğŸ’¡ Tips:**

* Start with defaults.
* Change one setting at a time.
* Match settings to task type.

---

## ğŸ  Homework

ğŸ§© Explore complete **handoff** module â€” every detail matters for your quiz!
ğŸ–¥ Enable **WSL:** [how to enable WSL](https://www.google.com/search?q=how+to+enable+wsl+in+windows+10)
ğŸ’» Install [Git Bash](https://git-scm.com/downloads)
ğŸ§  Install [Gemini CLI](https://github.com/google-gemini/gemini-cli)
ğŸ¤– Install [Qwen CLI](https://github.com/QwenLM/qwen-code)
ğŸ§© Explore [OpenAI Codex](https://github.com/openai/codex)
ğŸ§± Explore [Spec-Kit-Plus](https://github.com/panaversity/spec-kit-plus)
ğŸ“– Read [AgentKit overview](https://openai.com/index/introducing-agentkit/)
